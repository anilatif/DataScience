# -*- coding: utf-8 -*-
"""Copy of Lab6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s38L4DNfuyuUdHRAZ_etnwNVNSGienw-
"""

import pandas as pd
import numpy as np

import numexpr
numexpr.set_num_threads(24)
# In[ ]:
import pandas as pd
data = pd.read_csv('Zip_Zhvi_SingleFamilyResidence.csv',encoding='ISO-8859-1' )

#drop the columns from 1996 because we're only looking at 1997 - present 
complete = data.drop(['1996-04', '1996-05', '1996-06', '1996-07', '1996-08', '1996-09',
                             '1996-10', '1996-11','1996-12'],axis=1)

#drop the rows with Nans 
#get a list of dates for the subset 
na_list = list(complete.columns)
na_list = na_list[7:]
complete = complete.dropna(subset=na_list)
complete

complete['RegionName'].value_counts()
#get the unique metro areas for arkansas
(complete.loc[complete['State']=='AR'])['Metro'].unique()
# In[ ]:
#get the subset for Hot Springs, Little Rock, Fayetteville, Searcy
arkansas = complete.loc[complete['Metro'].isin(['Hot Springs', 'Little Rock-North Little Rock-Conway', 'Fayetteville-Springdale-Rogers', 'Searcy'])]
arkansas

#get one row of observations for each metro area

arkansas_grouped=arkansas.groupby(['Metro']).mean()
arkansas_grouped

#drop regionID, regionName, Sizerank,
arkansas_grouped=arkansas_grouped.drop(['RegionID', 'RegionName','SizeRank'], axis=1)

#transform the dataframe into Region ID ,time , value 
arkansas_grouped['Metro'] = ['Fayetteville', 'Hot Springs', 'Little Rock', 'Searcy']
ark_melt = pd.melt(arkansas_grouped, id_vars=['Metro'])

#convert date to date object
ark_melt['variable']=pd.to_datetime(ark_melt['variable'], format='%Y-%m')

#create timeseries for Hot Springs, Little Rock, Fayetteville, Searcy 
hotsprings = ark_melt.loc[ark_melt['Metro']=='Hot Springs']
lilrock = ark_melt.loc[ark_melt['Metro']=='Little Rock']
fayetteville = ark_melt.loc[ark_melt['Metro']=='Fayetteville']
searcy = ark_melt.loc[ark_melt['Metro']=='Searcy']

#use https://matplotlib.org/devdocs/gallery/subplots_axes_and_figures/subplots_demo.html to plot multiple graphs on the same plot
from matplotlib import pyplot as plt

#plot each of the metro areas on their own plot 
fig, axs = plt.subplots(2, 2)
axs[0, 0].plot(hotsprings['variable'], hotsprings['value'])
axs[0, 0].set_title('Hot Springs')
axs[0, 1].plot(lilrock['variable'], lilrock['value'], 'tab:orange')
axs[0, 1].set_title('Little Rock')
axs[1, 0].plot(fayetteville['variable'], fayetteville['value'], 'tab:green')
axs[1, 0].set_title('Fayetteville')
axs[1, 1].plot(searcy['variable'], searcy['value'], 'tab:red')
axs[1, 1].set_title('Searcy')
fig.tight_layout(pad=1.0)
#for ax in axs.flat:
 #   ax.set(xlabel='x-label', ylabel='y-label')

plt.show()

#https://stackoverflow.com/questions/49100090/python-line-plot-multiple-time-series-on-same-plot to plot all lines on same graph
ark_melt_piv = ark_melt.pivot(index='variable', columns='Metro', values='value')
ark_melt_piv.plot()
plt.xlabel('Year')
plt.ylabel('Average Housing Value (in dollars)')
plt.title('Arkansas Metro Areas: Housing Value over Time')

#remove 'Metro' from legend
plt.legend(title='')
plt.show()

#save cleaned dataset to csv 
complete.to_csv('complete.csv')

#look at the unique metro areas
complete['Metro'].unique()
# In[ ]:
#add extra data 
extra_data = pd.read_fwf('https://www.bls.gov/web/metro/ssamatab1.txt')
#extra_data.to_csv('extra_data.csv')
extra_data

extra_data.head()
#rename columns
extra_data.columns = ['Metro_meta', 'Year', 'Month', 'Labor_Force', 'Employment', 'Unemployment', 'Unemployment_rate']
extra_data.head()
#remove the first 4 rows and last 3 (citation and headers)
extra_data=extra_data[4:-3]

#reformat the date column 
extra_data['date'] = extra_data['Year'] + "-" + extra_data['Month']
extra_data.head()

#clean up metro_meta
metro_clean = []
for i in range(4,142564):
  metro_clean.append(extra_data['Metro_meta'][i][40:-8])

#add cleaned metro to the df
extra_data['Metro']=metro_clean
extra_data
# In[ ]:
import matplotlib.pyplot as plt
import seaborn
#which metro areas have the lowest mean unemployment rate?
extra_data['Unemployment_rate'] = extra_data['Unemployment_rate'].astype('float')
metro_grouped=extra_data.groupby(['Metro']).mean()
metro_grouped
metro_grouped= (metro_grouped['Unemployment_rate'].sort_values(ascending=False))


#top 10 metro areas with highest unemployment rates?
top10 = metro_grouped[:10]
fig = plt.figure(figsize=(13, 9))
seaborn.barplot(x=top10.index, y=top10.values)
plt.title("Top 10 Highest Average Unemployment Rates overall")
plt.ylabel("Unemployment Rate")
plt.show()
# In[ ]:
#top5.values

#what is the average housing price over time 
aggregated_date = complete
aggregated_date = aggregated_date.drop(['RegionID', 'City', 'State', 'CountyName','Metro', 'RegionName', 'SizeRank'], axis=1)
fig = plt.figure(figsize=(12, 9))
aggregated_date = aggregated_date.aggregate("mean")
aggregated_date.index = pd.to_datetime(aggregated_date.index)

#aggregated_date
plt.plot(aggregated_date)
plt.title("Average Median Housing Value Over Time")
plt.ylabel("Average Median Housing Value (in dollars)")

#removing x ticks because the axis is too congested
#https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot 
#plt.tick_params(
 #   axis='x',          # changes apply to the x-axis
  #  which='both',      # both major and minor ticks are affected
  #  bottom=False,      # ticks along the bottom edge are off
  #  top=False,         # ticks along the top edge are off
  #  labelbottom=False) # labels along the bottom edge are off

#annotating max
#https://stackoverflow.com/questions/43374920/how-to-automatically-annotate-maximum-value-in-pyplot/43375405

#shortened set to get the first max 
first_max = aggregated_date[:150]
ymax_1 = max(first_max)
xpos_1 = first_max[first_max==ymax_1].index
xpos_1_text=(((((((str(xpos_1).replace("Index", " ")).replace('['," ")).replace(']' ,' ').replace(', dtype=\'object\'', ' ')).replace(
    '(', ' ')).replace(')','')).replace('\'', '')))

plt.annotate("October 2006", xy=(xpos_1, ymax_1), xytext=(xpos_1, ymax_1+13000),
            arrowprops=dict(facecolor='green', shrink=0.05),
            )
#now find the value of that dip, take the next half of the observations
first_min = aggregated_date[150:]
ymin = min(first_min)

xpos_2 = first_min[first_min==ymin].index

plt.annotate("February 2012", xy=(xpos_2, ymin), xytext=(xpos_2,ymin-15000),
            arrowprops=dict(facecolor='red', shrink=0.05),
            )
plt.show()
# In[ ]:
#get state level information
states = []
#from the metro meta data, split on the comma, isolate the state values, and then isolate the last two letter value as the state again
for i in range(4,142564):
  states.append((((extra_data['Metro_meta'][i].split(","))[1][:-3]).strip())[:2])
states

extra_data['States']=states

#aggregate based on states
agg_state = (extra_data.drop(['Metro_meta','Year','Month','Labor_Force',
                              'Employment',
                              'Unemployment','date',	'Metro'], axis=1))
agg_state=agg_state.groupby('States').mean()

# In[ ]:

type(list(agg_state.index))

import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt

fig = go.Figure(px.choropleth(locations=agg_state.index, locationmode="USA-states", color= agg_state.values, scope="usa"))

fig.update_layout(title="Average Unemployment Rate by State")

fig.data[-1].showlegend = True
fig.update_layout(legend_title='<b> Unemployment Rate </b>')
fig.show()
# In[ ]:
#plot avg unemployment by county
#first figure out how to ID county -- use FIPS code -- this is in the metro metadata
#isolate the fips codes
#get state level information
fips = []
#from the metro meta data, isolate just the fips 
for i in range(4,142564):
  fips.append(str((extra_data['Metro_meta'][i][2:7]).strip()))

fip_table = extra_data
fip_table['fips'] = fips
fip_table['Unemployment_rate']=fip_table['Unemployment_rate'].astype('float')

#aggregate based on fips
agg_fips = (fip_table.drop(['Metro_meta','Year','Month','Labor_Force',
                              'Employment','States',
                              'Unemployment','date',	'Metro'], axis=1))
agg_fips=round(fip_table.groupby('fips').mean())
agg_fips.index

# In[ ]:



#preprocess the original complete dataset
#drop the other vars for melting
complete_clean=complete.drop(['Metro', 'RegionID','City', 'State','SizeRank', 'CountyName'], axis=1)

#melt dataset
complete_melt = pd.melt(complete_clean, id_vars=['RegionName'])
#format the date 
complete_melt['variable']=pd.to_datetime(complete_melt['variable'], format='%Y-%m')

complete_melt.head()
#add back the metro and other vars for matching
complete_join = pd.merge(complete_melt, complete[['Metro', 'City', 'State', 'SizeRank', 'RegionID','RegionName']], how='inner', on='RegionName')

complete_join

jointable=extra_data
jointable['date']= pd.to_datetime(jointable['date'])
dfjoined=pd.merge(complete_join, jointable,  how='left', right_on=['Metro','date'], left_on = ['Metro','variable'])
dfjoined=dfjoined.dropna()
len(dfjoined)

dfjoined.tail()
# In[ ]:
#drop extra columns from merge 
dfjoined=dfjoined.drop(['Metro_meta', 'Year','Month', 'date'], axis=1)
dfjoined.head()
# In[ ]:
#import prophet 
from fbprophet import Prophet
#rename dates and values to ds and y
dfjoined=dfjoined.rename(columns={"Metro_left": "Metro", "variable": "ds", "value": "y"})

#change strings to float type, remove commas 
dfjoined['Labor_Force']=dfjoined['Labor_Force'].str.replace(",","").astype(float)
dfjoined['Employment']=dfjoined['Employment'].str.replace(",","").astype(float)
dfjoined['Unemployment']=dfjoined['Unemployment'].str.replace(",","").astype(float)
# In[ ]:
#break up into a sets of data for each zipcode
df_sets = {}
for i in set(dfjoined['RegionName']):
  print(i)
  df = dfjoined[dfjoined['RegionName'] == i]
  df = df.reset_index(level=0)
  df_sets[i]= df

#verify 
print("the length is" , (len(df_sets)))

# In[ ]:

#create a model for each zipcode 
prophet_models = {}
for i,df in df_sets.items():
  prophet_models[i] = Prophet(interval_width=0.95)
  #add additional variables to the model (unemployment rate)
  prophet_models[i].add_regressor('Unemployment_rate')
  print(len(prophet_models))
  prophet_models[i].fit(df)
print ("the length is", len(prophet_models))

#get future date for each set, predict for 2018
#data ends in 11/2016, to predict for 2018, we need 1+12+12 months in advance 
future_dates = {}
for i,df in prophet_models.items():
  print (len(future_dates))
  future_dates[i] = prophet_models[i].make_future_dataframe(periods=13, freq='M')
# In[ ]:
#check out some of the results
#future_dates.keys()
#look at one of the zip codes 
#future_dates[79936]

import matplotlib.pyplot as plt
#assumed that if I plotted multople graphs there would be a general trendline/some general seasonality 

#plt.plot((dfjoined.loc[dfjoined['RegionName']==11206]['ds']),  (dfjoined.loc[dfjoined['RegionName']==11206]['Unemployment_rate']).astype(float))
#plt.plot((dfjoined.loc[dfjoined['RegionName']==60657]['ds']),  (dfjoined.loc[dfjoined['RegionName']==60657]['Unemployment_rate']).astype(float))
#plt.plot((dfjoined.loc[dfjoined['RegionName']==11235]['ds']),  (dfjoined.loc[dfjoined['RegionName']==11235]['Unemployment_rate']).astype(float))
#plt.plot((dfjoined.loc[dfjoined['RegionName']==77429]['ds']),  (dfjoined.loc[dfjoined['RegionName']==77429]['Unemployment_rate']).astype(float))

#plotting all the graphs just gives you a mess
#for i in dfjoined['RegionName']:
 # plt.plot((dfjoined.loc[dfjoined['RegionName']==i]['ds']),  (dfjoined.loc[dfjoined['RegionName']==i]['Unemployment_rate']).astype(float))
# In[ ]:
#for each set, get the unemployment values for some forecasting to prepare variable as regressor 
unemploy_sets = {}
for i in set(dfjoined['RegionName']):
  print(len(unemploy_sets))
  df = dfjoined[dfjoined['RegionName'] == i]
  df = df.reset_index(level=0)
  unemploy_sets[i]= df

print("the length is" , (len(unemploy_sets)))
# In[ ]:
#predict the next 13 months for unemployment rate 
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf  
from scipy.stats import uniform 

#for each set, take the log of the unemployment value -- induce stationarity 
log_unemploy_sets = []
for key in unemploy_sets:
  log_unemploy_sets.append(np.log(unemploy_sets[key]['Unemployment_rate'].astype(float)))
log_unemploy_sets

# In[ ]:

# In[ ]   
#install pmdarima to handle automatic parameter setting in arima 
#!pip install pmdarima

import pmdarima as pm
#https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.ARIMA.html
#do a random fit arima model for each element in unemploy_log_diffset

random_fits = []
for i in range(0, len(log_unemploy_sets)):
  random_fits.append(np.exp((pm.auto_arima(log_unemploy_sets[i], start_p=1, start_q=1, max_p=3, max_q=3, m=12,
  n_jobs=-1,
  error_action='ignore',
  suppress_warnings=True, #memory issues cause warnings 
  stepwise=False,
  paralletl = True, 
  #information_criterion='bic',
  random=True, random_state=42, n_fits=10)).predict(n_periods=13)))
  print (i)

# In[ ]:
random_fits 

#add back to the data to prepare for prophet 
from pandas.core.common import flatten

future_regressor_sets = {}
for i, j in zip((set(future_dates)),(range(0,len(random_fits)))):
     future_regressor_sets[i] = list(flatten(([unemploy_sets[i]['Unemployment_rate']],random_fits[j])))

test=future_dates
future_regressor_sets
# In[ ]:

#put the data in a format the prophet will understand -- ds and unemployment rates including future values
for key in test:
  test[key]['Unemployment_rate']=pd.Series(future_regressor_sets[key])
  # In[ ]:


#use prophet to get those predictions/forecasts 
test_f={}
for i in test:
  test_f[i] = prophet_models[i].predict(test[i])
  print (len(test_f))
# In[ ]:
#forecasts
 test_f
 # In[ ]:

#create the forecast 
#forecasts = {}
#for i,df in df_sets.items():
  #forecasts[i] = prophet_models[i].predict(future_dates[i])
 # print(i)
  #print(forecasts[i][['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())
#forecast_regressor = {}
#for i,df in df_sets.items():
 # forecast_regressor[i] = prophet_models[i].predict(combined_dict[i][0])

#forecasts

#plot the models 
import matplotlib.pyplot as plt
for i,model in prophet_models.items():
  model.plot(test_f[i], uncertainty=True)
  plt.ylabel('Median Housing Average (in dollars)')
  plt.xlabel('Date')
  plt.title(i)
plt.show()

# In[ ]:

#how do I know which ones are the best models?
#find the Mean Absolute Percentage Error -- pick the smallest one 

MAPE_all= {}
for key in unemploy_sets:
  y_value = unemploy_sets[key]['y'].astype('float')
  y_hat = test_f[key]['yhat'].astype('float')
  MAPE_all[key]=(np.mean(np.abs((y_value-y_hat)/y_value))*100)
  
# In[ ]:

#find the smoothest graph
#https://www.w3resource.com/python-exercises/dictionary/python-data-type-dictionary-exercise-1.php
#for dictionary sort 
import operator 
sorted_dict = sorted(MAPE_all.items(), key=operator.itemgetter(1))

#get the top 5 
print ("RegionName", "MAPE")
sorted_dict[:10]
# In[ ]:
first_10 = sorted_dict[:10]
first_10[0][0]
sorted_dict[:10][1][0]

# In[ ]:
#plot the 10 best models
first_10_keys = []
for i in range(0, len(first_10)):
  first_10_keys.append(first_10[i][0])
first_10_keys
# =============================================================================
# 
# =============================================================================
for i in first_10_keys:
  prophet_models[i].plot(test_f[i],uncertainty=True)
  plt.ylabel('Median Housing Average (in dollars)')
  plt.xlabel('Date')
  plt.title(i)
plt.show
